{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline import Pipeline, Task\n",
    "from fetch_data import fetch_arxiv_papers\n",
    "from clean_data import get_filtered_sections_papers\n",
    "from condense_data import condensed_papers\n",
    "from qa_pairs_generation import generate_all_qa_pairs\n",
    "\n",
    "pipeline = Pipeline(\"dataset_creation\", [\n",
    "    Task(fetch_arxiv_papers, {\"query\": \"deep learning\", \"max_results\": 500}, False),\n",
    "    Task(get_filtered_sections_papers, {}, False),\n",
    "    Task(condensed_papers, {\"amount\": 100}, False),\n",
    "    Task(generate_all_qa_pairs, {\"max_retry\": 5, \"amount\": 100}, False)\n",
    "])\n",
    "\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_train import gpu_train\n",
    "from pipeline import Pipeline\n",
    "\n",
    "gpu_train(Pipeline(\"dataset_creation\").get_data_from_step(3), \"unsloth/Llama-3.2-3B-Instruct-unsloth-bnb-4bit\", 1024, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def pick_random_qa_pairs(qa_pairs, sample_amount):\n",
    "    return random.sample(qa_pairs, sample_amount) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm_studio_caller import call_llm\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_base_model_answers(qa_pairs, repetition):\n",
    "    for qa in tqdm(qa_pairs):\n",
    "        qa[\"llm_output\"] = []\n",
    "        for _ in range(repetition):\n",
    "            qa[\"llm_output\"].append(call_llm(sys_prompt=\"Answer concisely.\", usr_prompt=qa[\"question\"], temperature=0.7))\n",
    "    \n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from use_fine_tunned_model import load_optimized_model, question_model\n",
    "\n",
    "def generate_finetuned_model_answers(qa_pairs, repetition):\n",
    "    model, tokenizer = load_optimized_model(\n",
    "        \"unsloth/Llama-3.2-3B-Instruct-unsloth-bnb-4bit\",\n",
    "        \"test\"\n",
    "    )\n",
    "    \n",
    "    for qa in tqdm(qa_pairs):\n",
    "        qa[\"llm_finetuned_output\"] = []\n",
    "        for _ in range(repetition):\n",
    "            qa[\"llm_finetuned_output\"].append(question_model(model, tokenizer, qa[\"question\"], \"Answer concisely.\"))\n",
    "\n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline import Pipeline, Task\n",
    "\n",
    "pipeline = Pipeline(\"evaluation\", [\n",
    "        Task(pick_random_qa_pairs, {\"sample_amount\": 5}, False),\n",
    "        Task(generate_base_model_answers, {\"repetition\": 3}, False),\n",
    "        Task(generate_finetuned_model_answers, {\"repetition\": 3}, False)\n",
    "    ],\n",
    "    initial_data=[qa for qas_pair in Pipeline(\"dataset_creation\").get_data_from_step(3).values() for qa in qas_pair]\n",
    ")\n",
    "\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sbert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def compute_sbert_similarities(dataset):\n",
    "    similarities_scores = []\n",
    "    for qa in dataset:\n",
    "        answer_emb = sbert_model.encode(qa[\"answer\"], convert_to_tensor=True)\n",
    "        llm_output_embs = [sbert_model.encode(llm_output, convert_to_tensor=True) for llm_output in qa[\"llm_output\"]]\n",
    "        llm_finetuned_output_embs = [sbert_model.encode(llm_output, convert_to_tensor=True) for llm_output in qa[\"llm_output\"]]\n",
    "        \n",
    "        scores = []\n",
    "        for emb1 in llm_finetuned_output_embs:\n",
    "            scores.append(util.pytorch_cos_sim(answer_emb, emb1).item())\n",
    "        \n",
    "        diff_scores = []\n",
    "        for emb1 in llm_output_embs:\n",
    "            for emb2 in llm_finetuned_output_embs:\n",
    "                diff_scores.append(util.pytorch_cos_sim(emb1, emb2).item())\n",
    "        \n",
    "        similarities_scores.append({\n",
    "            \"question\": qa[\"question\"],\n",
    "            \"avg_score\": np.mean(scores),\n",
    "            \"avg_diff_score\": np.mean(diff_scores)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(similarities_scores).sort_values(\"avg_score\", ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from pipeline import Pipeline\n",
    "\n",
    "qa_pairs = Pipeline(\"evaluation\").get_data_from_step(2)\n",
    "similarities_scores = compute_sbert_similarities(qa_pairs)\n",
    "\n",
    "def plot_similarity(df):\n",
    "    if df.empty:\n",
    "        print(\"Warning: No similarity scores computed!\")\n",
    "        return\n",
    "\n",
    "    fig = px.bar(\n",
    "        df.melt(id_vars=[\"question\"], value_vars=[\"avg_score\", \"avg_diff_score\"],\n",
    "                var_name=\"Model Type\", value_name=\"Similarity Score\"),\n",
    "        x=\"Similarity Score\",\n",
    "        y=\"question\",\n",
    "        orientation=\"h\",\n",
    "        color=\"Model Type\",\n",
    "        color_discrete_map={\"avg_score\": \"green\", \"avg_diff_score\": \"blue\"},\n",
    "        title=\"Fine-Tuned vs Non-Fine-Tuned Model Sbert Similarities\",\n",
    "        labels={\"question\": \"Question\", \"Similarity Score\": \"Similarity Score\"},\n",
    "        text_auto=\".2f\",\n",
    "        hover_data={\"question\": False}\n",
    "    )\n",
    "    \n",
    "    fig.update_traces(\n",
    "        textposition=\"inside\",\n",
    "        customdata=df[[\"avg_score\", \"avg_diff_score\"]].values,\n",
    "        hovertemplate=\"<b>→ Average Similarity Score:</b> %{customdata[0]:.2f}<br>\"\n",
    "                      \"<b>→ Average Similarity Score Diff:</b> %{customdata[1]:.2f}<br>\"\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        yaxis=dict(\n",
    "            title=\"Question\",\n",
    "            tickmode=\"array\",\n",
    "            tickfont=dict(size=12),\n",
    "            automargin=True\n",
    "        ),\n",
    "        barmode=\"group\",\n",
    "        width=2250,\n",
    "        height=900\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "plot_similarity(similarities_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
